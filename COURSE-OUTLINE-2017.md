# Course Outline of 2017

## Week 1

* **Chapter 1**
	* Read completely (level R)

* **Chapter 2**
	* Read completely (level RU)
	* You have to be able to understand and build regular expressions for given tasks
	* You have to be able to understand and build an FSA
	* Exercises:
		* Do all five crosswords under [http://regexcrossword.com/challenges/intermediate/puzzles/1](http://regexcrossword.com/challenges/intermediate/puzzles/1) i.e. with [12345] at the end	
		* Attempt to do [http://regexcrossword.com/challenges/experienced/puzzles/4](http://regexcrossword.com/challenges/experienced/puzzles/4) and [http://regexcrossword.com/challenges/experienced/puzzles/5](http://regexcrossword.com/challenges/experienced/puzzles/5)

* **Chapter 3**
	* Read at level RU: Intro, 3.1, 3.2, 3.8, 3.9, Summary
	* Read at level R: Notes
	* Exercises:
		* Do exercise 3.1 (“Give examples…”).		



## Week 2

* **Chapter 4**
	* Read Intro (RU), 4.1 (RU), 4.2 (RU), 4.3 (RU), 4.4 (RU), 4.5 (R), 4.6 (R), 4.7.intro (R), 4.8 (R), 4.9 (R), 4.10 (RU), 4.11 (R), Summary (RU), Notes (R)
	* Note: As this is a quite intense week, the subject of smoothing has been set to the lower requirement grade. However, you should note that application of a smoothing method is usually of crucial importance.

* **Chapter 5**
	* Read Intro (RU), 5.1 (RU), 5.2 (RU), 5.3 (RU), 5.4 (R), 5.5 (RU), 5.6 (R), 5.7 (RU), 5.8 (RU), Summary (RU), Notes (R)
	* Exercises:
		* Do exercise 5.2. (“Use the Penn Treebank tagset…”). Bring a print to the classroom for comparison and discussion of problematic aspects
		* Do exercise 5.7 ("Recall that the Church (1988) tagger...").. If you cannot come up with an actual example, at least describe how you would go about finding one.	


## Week 3

* NOT IN J&M: Introduction to Machine Learning
	* General: [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (RU), 4 (RU), 5 (RU)
	* Decision Trees: Powerpoint Lewicki, see attachment
		* Read completely (RU)
	* Naive Bayes: [https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
		* Read Intro (RU), 1 (RU), 2 (RU), 5 (R)
	* Kernel methods: [http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
		* Read Completely (RU, but I don't expect you to reproduce the mathematics)
	* Nearest Neigbour: [http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf](http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf)
		* Read completely (RU)
	* Clustering: [https://en.wikipedia.org/wiki/Cluster_analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
		* Read Intro (RU), 1 (RU), 2 (R), 4 (R)
	* Deep Learning: [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (R), 4 (R), 5 (RU), 8 (RU)
	* Exercise: Do the Machine Learning exercise below.							


* **Chapter 6**
	* Read Intro (RU), 6.1 (RU), 6.2 (RU), 6.3 (RU), 6.4 (RU), 6.5 (R),  6.6 (intro+6.6.1 RU; 6.6.2+6.6.3 R), 6.7 (R), 6.8 (RU), Summary (RU) , Notes (R)
	* Exercises:
		* If we look at the Viterbi algorithm, we see that the probability of state at a given position is calculated n the basis of the preceding n states. However, it is claimed that the algorithm takes into account the whole sequence. Explain in your own words (at most 100) how the probability is influenced by the rest of the sequence, i.e. both the positions more than n back and the following positions.
		* Explain in your own words (at most 50) how the EM algorithm works. I don’t mean the mathematics, but the underlying concept.

* Machine Learning exercise
	* For instructions, see ML_exercise.pdf. The other file is data for the exercise.
		* ML_Exercise.pdf
		* lobtlb_nnvb.csv

## Week 4





## Week 5





## Week 6






## Week 7






## Week 8





## Week 9





## Week 10





## Week 11





## Week 12





## Week 13





## Week 14