# Course Outline of 2017

## Week 1

* **Chapter 1**
	* Read completely (level R)

* **Chapter 2**
	* Read completely (level RU)
	* You have to be able to understand and build regular expressions for given tasks
	* You have to be able to understand and build an FSA
	* Exercises:
		* Do all five crosswords under [http://regexcrossword.com/challenges/intermediate/puzzles/1](http://regexcrossword.com/challenges/intermediate/puzzles/1) i.e. with [12345] at the end	
		* Attempt to do [http://regexcrossword.com/challenges/experienced/puzzles/4](http://regexcrossword.com/challenges/experienced/puzzles/4) and [http://regexcrossword.com/challenges/experienced/puzzles/5](http://regexcrossword.com/challenges/experienced/puzzles/5)

* **Chapter 3**
	* Read at level RU: Intro, 3.1, 3.2, 3.8, 3.9, Summary
	* Read at level R: Notes
	* Exercises:
		* Do exercise 3.1 (“Give examples…”).		



## Week 2

* **Chapter 4**
	* Read Intro (RU), 4.1 (RU), 4.2 (RU), 4.3 (RU), 4.4 (RU), 4.5 (R), 4.6 (R), 4.7.intro (R), 4.8 (R), 4.9 (R), 4.10 (RU), 4.11 (R), Summary (RU), Notes (R)
	* Note: As this is a quite intense week, the subject of smoothing has been set to the lower requirement grade. However, you should note that application of a smoothing method is usually of crucial importance.

* **Chapter 5**
	* Read Intro (RU), 5.1 (RU), 5.2 (RU), 5.3 (RU), 5.4 (R), 5.5 (RU), 5.6 (R), 5.7 (RU), 5.8 (RU), Summary (RU), Notes (R)
	* Exercises:
		* Do exercise 5.2. (“Use the Penn Treebank tagset…”). Bring a print to the classroom for comparison and discussion of problematic aspects
		* Do exercise 5.7 ("Recall that the Church (1988) tagger...").. If you cannot come up with an actual example, at least describe how you would go about finding one.	


## Week 3

* NOT IN J&M: Introduction to Machine Learning
	* General: [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (RU), 4 (RU), 5 (RU)
	* Decision Trees: Powerpoint Lewicki, see attachment
		* Read completely (RU)
	* Naive Bayes: [https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
		* Read Intro (RU), 1 (RU), 2 (RU), 5 (R)
	* Kernel methods: [http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
		* Read Completely (RU, but I don't expect you to reproduce the mathematics)
	* Nearest Neigbour: [http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf](http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf)
		* Read completely (RU)
	* Clustering: [https://en.wikipedia.org/wiki/Cluster_analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
		* Read Intro (RU), 1 (RU), 2 (R), 4 (R)
	* Deep Learning: [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (R), 4 (R), 5 (RU), 8 (RU)
	* Exercise: Do the Machine Learning exercise below.							


* **Chapter 6**
	* Read Intro (RU), 6.1 (RU), 6.2 (RU), 6.3 (RU), 6.4 (RU), 6.5 (R),  6.6 (intro+6.6.1 RU; 6.6.2+6.6.3 R), 6.7 (R), 6.8 (RU), Summary (RU) , Notes (R)
	* Exercises:
		* If we look at the Viterbi algorithm, we see that the probability of state at a given position is calculated n the basis of the preceding n states. However, it is claimed that the algorithm takes into account the whole sequence. Explain in your own words (at most 100) how the probability is influenced by the rest of the sequence, i.e. both the positions more than n back and the following positions.
		* Explain in your own words (at most 50) how the EM algorithm works. I don’t mean the mathematics, but the underlying concept.

* Machine Learning exercise
	* For instructions, see ML_exercise.pdf. The other file is data for the exercise.
		* ML_Exercise.pdf
		* lobtlb_nnvb.csv

## Week 4

* **Chapter 7**
	* Read Intro (RU), 7.1 (RU), 7.3 (RU), 7.4 (RU), Summary (RU)
		* You do not need to be able to understand the exact mathematics in 7.4
	* Excercises
		* Do Exercise 7.2 ("Translate the pronunciations...")	

* **Chapter 8**
	* Read completely (RU)
	* Excercises:
		* Look at the text in https://books.google.pl/books?id=urBjAAAAcAAJ&pg=PP1&img=1&zoom=3&hl=en&sig=ACfU3U1jdzfl6IHA5gFYN9yg_NOJ2uB3hw&ci=69%2C168%2C736%2C1295&edge=0
		* Or, if this does not work, see the attached screen print (Tracts.jpg).
		* Describe what needs to be done to get this text read out loud, e.g. to a blind person. Where useful, show intermediate representations (not necessarily for the complete text, but for parts of the text whch exemplify the representation well).


## Week 5

* **Chapter 9**
	* Read completely (RU)
		* You do not need to be able to understand the exact mathematics in 9.3 and 9.4
	* Exercises
		* Do exercise 9.1
		* When people speak fast, they start leaving out bits of words (reduction). If we want our speech recognizer to still recognize the words spoken, even though they are reduced, which elements of the architecture should we adapt, and how?	



## Week 6

* **Chapter 12**
	* Read Intro(RU), 12.1 (RU), 12.2 (RU), 12.3 (RU), 12.4 (RU), 12.5 (RU), 12.6 (RU), 12.7.intro (RU), 12.7.1 (RU), 12.8 (RU), Summary (RU), Notes (R)
	* Exercises:
		* Do Exercise 12.5 ("How many types of NPs...")
		* Do Exercise 12.6 ("Assume a grammar...")
		* Do Exercise 12.7 ("Does your solution...")
		* Do Exercise 12.10 ("Page XXX discussed the need...")
		* Now combine 12.6 and 12.10 to allow relative clauses to start with an Wh-NP.

* **Chapter 16 (BEWARE: in the draft, this is Chapter 15)**
	* Read Intro (RU), 16.1 (RU), 16.3 (R), Summary (RU)




## Week 7

* **Chapter 13**
	* Read Completely (RU)
	* Exercises
		* Note: For A, B and C, I am only asking for an explanation in natural language (English). However, if you prefer, you can also adjust the pseudocode (Figures 13.10 and 13.13). For D, don't even try to produce pseudocode, but stick with an explanation.
		* A) Explain what you would have to do to the CKY algorithm so that it can accept grammars that contain unit productions.
		* B) Explain what you would have to do to the CKY algorithm so that it can also handle grammars with right hand sides with more than two non-terminals (and without first transforming the grammar). You do not have to write pseudocode, just explain.
		* C) Explain how you would have to augment the Earley algorithm of Figure 13.13 to enable parse trees to be retrieved from the chart.
		* D) Suggest a way to alter the Earley algorithm so that it makes better use of bottom-up information to reduce the number of useless predictions.

* **Chapter 15 (BEWARE: in the draft, this is Chapter 16)**
	* Read Intro (RU), 15.1 (RU), 15.2 (RU), 15.3 (RU), 15.5.intro (R), 15.5.1 (R),  15.6.intro (RU), 15.6.1 (R), Summary (RU), Notes (R)
	* Exercises
		* A) Do Exercise 15.2. ("Consider the following examples from the Berkeley Restaurant Project..."). You do not have to write a full grammar., but assume that all the rules in Section 15.3 are already present. However, if you feel the need to adjust such a rule, you do have to provide the adjusted version.
		* B) Do Exercise 15.3 (" Draw a picture of the subsumption lattice...").

## Week 8





## Week 9





## Week 10





## Week 11





## Week 12





## Week 13





## Week 14