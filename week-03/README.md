# Week 3

* Chapter 6 - Hidden Markov and Maximum Entropy Models:
		* Read Intro (RU), 6.1 (RU), 6.2 (RU), 6.3 (RU), 6.4 (RU), 6.5 (R),  6.6 (intro+6.6.1 RU; 6.6.2+6.6.3 R), 6.7 (R), 6.8 (RU), Summary (RU) , Notes (R)

* Introduction to Machine Learning
		* General:  https://en.wikipedia.org/wiki/Machine_learning
				* Read Intro (RU), 1 (RU), 2 (R), 3 (RU), 4 (RU), 5 (RU)
		* Decision Trees: Powerpoint Lewicki, see attachment	
				* Read completely (RU)
		* Naive Bayes: https://en.wikipedia.org/wiki/Naive_Bayes_classifier
				* Read Intro (RU), 1 (RU), 2 (RU), 5 (R)
		* Kernel methods: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html	
				* Read Completely (RU, but I don't expect you to reproduce the mathematics)
		* Nearest Neigbour: http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf
				* Read completely (RU)
		* Clustering: https://en.wikipedia.org/wiki/Cluster_analysis
				* Read Intro (RU), 1 (RU), 2 (R), 4 (R)
		* Deep Learning: https://en.wikipedia.org/wiki/Deep_learning
				* Read Intro (RU), 1 (RU), 2 (R), 3 (R), 4 (R), 5 (RU), 8 (RU)										



## Exercises

* If we look at the Viterbi algorithm, we see that the probability of a state at a given position is calculated on the basis of the preceding n states. However, it is claimed that the algorithm takes into account the whole sequence. Explain in your own words (at most 100) how the probability is influenced by the rest of the sequence, i.e. both the positions more than n back and the following positions.
* Explain in your own words (at most 50) how the EM algorithm works. I donâ€™t mean the mathematics, but the underlying concept.
* Do the Machine Learning exercise.