# Course Outline of 2017

## Week 1

* **Chapter 1**
	* Read completely (level R)

* **Chapter 2**
	* Read completely (level RU)
	* You have to be able to understand and build regular expressions for given tasks
	* You have to be able to understand and build an FSA
	* Exercises:
		* Do all five crosswords under [http://regexcrossword.com/challenges/intermediate/puzzles/1](http://regexcrossword.com/challenges/intermediate/puzzles/1) i.e. with [12345] at the end	
		* Attempt to do [http://regexcrossword.com/challenges/experienced/puzzles/4](http://regexcrossword.com/challenges/experienced/puzzles/4) and [http://regexcrossword.com/challenges/experienced/puzzles/5](http://regexcrossword.com/challenges/experienced/puzzles/5)

* **Chapter 3**
	* Read at level RU: Intro, 3.1, 3.2, 3.8, 3.9, Summary
	* Read at level R: Notes
	* Exercises:
		* Do exercise 3.1 (“Give examples…”).		



## Week 2

* **Chapter 4**
	* Read Intro (RU), 4.1 (RU), 4.2 (RU), 4.3 (RU), 4.4 (RU), 4.5 (R), 4.6 (R), 4.7.intro (R), 4.8 (R), 4.9 (R), 4.10 (RU), 4.11 (R), Summary (RU), Notes (R)
	* Note: As this is a quite intense week, the subject of smoothing has been set to the lower requirement grade. However, you should note that application of a smoothing method is usually of crucial importance.

* **Chapter 5**
	* Read Intro (RU), 5.1 (RU), 5.2 (RU), 5.3 (RU), 5.4 (R), 5.5 (RU), 5.6 (R), 5.7 (RU), 5.8 (RU), Summary (RU), Notes (R)
	* Exercises:
		* Do exercise 5.2. (“Use the Penn Treebank tagset…”). Bring a print to the classroom for comparison and discussion of problematic aspects
		* Do exercise 5.7 ("Recall that the Church (1988) tagger...").. If you cannot come up with an actual example, at least describe how you would go about finding one.	


## Week 3

* NOT IN J&M: Introduction to Machine Learning
	* General: [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (RU), 4 (RU), 5 (RU)
	* Decision Trees: Powerpoint Lewicki, see attachment
		* Read completely (RU)
	* Naive Bayes: [https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
		* Read Intro (RU), 1 (RU), 2 (RU), 5 (R)
	* Kernel methods: [http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
		* Read Completely (RU, but I don't expect you to reproduce the mathematics)
	* Nearest Neigbour: [http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf](http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf)
		* Read completely (RU)
	* Clustering: [https://en.wikipedia.org/wiki/Cluster_analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
		* Read Intro (RU), 1 (RU), 2 (R), 4 (R)
	* Deep Learning: [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)
		* Read Intro (RU), 1 (RU), 2 (R), 3 (R), 4 (R), 5 (RU), 8 (RU)
	* Exercise: Do the Machine Learning exercise below.							


* **Chapter 6**
	* Read Intro (RU), 6.1 (RU), 6.2 (RU), 6.3 (RU), 6.4 (RU), 6.5 (R),  6.6 (intro+6.6.1 RU; 6.6.2+6.6.3 R), 6.7 (R), 6.8 (RU), Summary (RU) , Notes (R)
	* Exercises:
		* If we look at the Viterbi algorithm, we see that the probability of state at a given position is calculated n the basis of the preceding n states. However, it is claimed that the algorithm takes into account the whole sequence. Explain in your own words (at most 100) how the probability is influenced by the rest of the sequence, i.e. both the positions more than n back and the following positions.
		* Explain in your own words (at most 50) how the EM algorithm works. I don’t mean the mathematics, but the underlying concept.

* Machine Learning exercise
	* For instructions, see ML_exercise.pdf. The other file is data for the exercise.
		* ML_Exercise.pdf
		* lobtlb_nnvb.csv

## Week 4

* **Chapter 7**
	* Read Intro (RU), 7.1 (RU), 7.3 (RU), 7.4 (RU), Summary (RU)
		* You do not need to be able to understand the exact mathematics in 7.4
	* Excercises
		* Do Exercise 7.2 ("Translate the pronunciations...")	

* **Chapter 8**
	* Read completely (RU)
	* Excercises:
		* Look at the text in https://books.google.pl/books?id=urBjAAAAcAAJ&pg=PP1&img=1&zoom=3&hl=en&sig=ACfU3U1jdzfl6IHA5gFYN9yg_NOJ2uB3hw&ci=69%2C168%2C736%2C1295&edge=0
		* Or, if this does not work, see the attached screen print (Tracts.jpg).
		* Describe what needs to be done to get this text read out loud, e.g. to a blind person. Where useful, show intermediate representations (not necessarily for the complete text, but for parts of the text whch exemplify the representation well).


## Week 5

* **Chapter 9**
	* Read completely (RU)
		* You do not need to be able to understand the exact mathematics in 9.3 and 9.4
	* Exercises
		* Do exercise 9.1
		* When people speak fast, they start leaving out bits of words (reduction). If we want our speech recognizer to still recognize the words spoken, even though they are reduced, which elements of the architecture should we adapt, and how?	



## Week 6

* **Chapter 12**
	* Read Intro(RU), 12.1 (RU), 12.2 (RU), 12.3 (RU), 12.4 (RU), 12.5 (RU), 12.6 (RU), 12.7.intro (RU), 12.7.1 (RU), 12.8 (RU), Summary (RU), Notes (R)
	* Exercises:
		* Do Exercise 12.5 ("How many types of NPs...")
		* Do Exercise 12.6 ("Assume a grammar...")
		* Do Exercise 12.7 ("Does your solution...")
		* Do Exercise 12.10 ("Page XXX discussed the need...")
		* Now combine 12.6 and 12.10 to allow relative clauses to start with an Wh-NP.

* **Chapter 16 (BEWARE: in the draft, this is Chapter 15)**
	* Read Intro (RU), 16.1 (RU), 16.3 (R), Summary (RU)




## Week 7

* **Chapter 13**
	* Read Completely (RU)
	* Exercises
		* Note: For A, B and C, I am only asking for an explanation in natural language (English). However, if you prefer, you can also adjust the pseudocode (Figures 13.10 and 13.13). For D, don't even try to produce pseudocode, but stick with an explanation.
		* A) Explain what you would have to do to the CKY algorithm so that it can accept grammars that contain unit productions.
		* B) Explain what you would have to do to the CKY algorithm so that it can also handle grammars with right hand sides with more than two non-terminals (and without first transforming the grammar). You do not have to write pseudocode, just explain.
		* C) Explain how you would have to augment the Earley algorithm of Figure 13.13 to enable parse trees to be retrieved from the chart.
		* D) Suggest a way to alter the Earley algorithm so that it makes better use of bottom-up information to reduce the number of useless predictions.

* **Chapter 15 (BEWARE: in the draft, this is Chapter 16)**
	* Read Intro (RU), 15.1 (RU), 15.2 (RU), 15.3 (RU), 15.5.intro (R), 15.5.1 (R),  15.6.intro (RU), 15.6.1 (R), Summary (RU), Notes (R)
	* Exercises
		* A) Do Exercise 15.2. ("Consider the following examples from the Berkeley Restaurant Project..."). You do not have to write a full grammar., but assume that all the rules in Section 15.3 are already present. However, if you feel the need to adjust such a rule, you do have to provide the adjusted version.
		* B) Do Exercise 15.3 (" Draw a picture of the subsumption lattice...").

## Week 8

* **Chapter 3**
	* Read 3.10 (RU), 3.11 (RU)
	* Exercises
		* Do exercise 3.10 (“Computing minimum edit distances by hand…”).

* **Chapter 5**
	* Read 5.9 (RU)
	* Exercises
		* Make a plan to do exercise 5.10 ("Compute a bigram grammar..."). Describe how you can derive the necessary probabilities from the corpus. Describe also which context information this represents and speculate what other context information might be useful for this task
		* Now do exercise 5.10. You do not have to extract bigram frequencies from a corpus yourself, but can use the file with bigram statistics at http://cls.ru.nl/staff/hvhalteren/bigrams.zip, taken from the Google N-Gram data. Although you are allowed to write a computer program and investigate all words, we suggest that you pick a few words that are close to “acress” in spelling (Word suggests "acres", "actress", "caress", "across" and "access") and compute the probabilities by hand.

* **Chapter 14**
	* Read Intro (RU), 14.1 (RU), 14.2 (RU), 14.3 (RU), 14.4 (RU), 14.5 (RU), 14.6 (RU), 14.7 (RU), 14.8 (RU), 14.9 (R),  14.10 (R), Summary (RU), Notes (R)
	* Exercises
		* A) Do exercise 14.2 ("Modify the algorithm for conversion to CNF...")
		* B) Explain what you would have to do to the probabilistic CKY algorithm so that it can accept grammars that contain unit productions.
		* C) Explain what you would have to do to the probabilistic CKY algorithm so that it can also handle grammars with right hand sides with more than two non-terminals (and without first transforming the grammar). You do not have to write pseudocode, just explain.		

## Week 9

* **Chapter 17**
	* Read completely (RU)
	* Exercises
		* Do Exercise 17.3 (in the draft 17.6): The rule which we gave as a translation for Example 17.24, is not a reasonable definition of what it means to be a vegetarian restaurant. 
		FORALL x VegetarianRestaurant(x) ⇒ Serves(x,VegetarianFood) 
		Give a FOL rule that better defines vegetarian restaurants in terms of what they serve.
		* Do Exercise 17.6 (in the draft 17.9): Give FOL translations for the following sentences that capture the temporal relationships between the events. 
		a. When Mary’s flight departed, I ate lunch. 
		b. When Mary’s flight departed, I had eaten lunch.




* **Chapter 18**

	* Read Intro (RU), 18.1 (RU), 18.2 (RU), 18.6 (RU), Summary (RU)
	* Exercises
		* Do Exercise 18.2 (not in draft): Develop a set of grammar rules and semantic attachments to handle so-called control verbs as in the following:
		1. Franco decided to leave.
		2. Nicolas told Franco to go to Frasca.
		The first of these is an example of subject control—Franco plays the role of the agent for both decide and leave. The second is an example of object control—there Franco is the person being told and the agent of the going event. The challenge in creating attachments for these rules is to properly incorporate the semantic representation of a single noun phrase into two roles.
		* Do Exercise 18.4: As noted in Chapter 17, the present tense in English can be used to refer to either the present or the future. However, it can also be used to express habitual behavior, as in the following:
		1. Flight 208 leaves at 3 o’clock.
		This could be a simple statement about today’s Flight 208, or alternatively it might state that this flight leaves at 3 o’clock every day. Create a FOL meaning representation along with appropriate semantic attachments for this habitual sense.


## Week 10

* **Chapter 19**
	* Read completely (RU)
	* Exercises
		* Consider the verbs "drink", "kiss", and "write". Using a dictionary definition and WordNet (http://wordnetweb.princeton.edu/perl/webwn), describe appropriate selectional restrictions on them, both in plain English and in terms of expected roles with their potential WordNet senses (for the senses, you can assume that the system has access to an inheritance mechanism). Next, collect six examples of each in the British National Corpus (http://corpus.byu.edu/bnc/), taking one example from each of the main text types (SPOKEN, FICTION, MAGAZINE, NEWSPAPER, NON-ACAD, ACADEMIC). Analyse the examples in terms of semantic roles, and discuss how well your restrictions applied. For the analysis, start with an automatic SRL-system (http://cogcomp.cs.illinois.edu/page/demo_view/srl), but correct its output where necessary.

* **Chapter 21**
	* Read Intro (RU), 21.1 (R), 21.3 (RU), 21.4 (R), 21.5 (RU), 21.6 (R),  21.7 (R), Summary (RU)
	* Exercises
		* Find a news article in English with mention of at least two persons, and submit it to the Stanford Core NLP system (http://nlp.stanford.edu:8080/corenlp/). Analyse the output, especially the coreference resolution graph.




## Week 11

* **Chapter 20**
	* Read Intro(RU), 20.1 (RU), 20.2 (RU), 20.3 (RU), **20.4 (R)**, 20.5 (RU), **20.6 (R)**, 20.7 (RU), 20.8 (RU), **20.9 (R)**, 20.10 (RU), Summary (RU), **Notes (R)**
	* Exercises: you can choose between two exercises: 
		* If you have computational skills and sufficient resources, investigate GloVe. Go to http://nlp.stanford.edu/projects/glove/ and download Glove.6B.zip. Write a program that, for a specific vector, finds the ten terms closest to that vector in cosine distance. Then start with word vectors. Take example words from WordNet in the hierarchy under food and ingest. Examine where the suggested words are in WordNet in relation to the query word. Examine also how the "quality" varies with the vector size. Continue with relations, e.g. water-drink+eat, examining different level in the hierarchy. Finally, briefly check compositionality.
		* If you are not up to the previous exercise, do Exercises 20.1 to 20.3 from the book (numbered 19.1 to 19.3 in the draft). If you did last week's exercise, you can reuse that corpus. About ten sentences should suffice, with length varying from 8 to 20 words. Use WordNet senses

* **Not in Jurafsky & Martin**
	* Read at level R
		* https://code.google.com/p/word2vec/
		* http://arxiv.org/pdf/1310.4546.pdf
		* http://nlp.stanford.edu/projects/glove/
		* http://nlp.stanford.edu/projects/glove/glove.pdf
	* NB GloVe appears to be better than word2vec, both in performance and in being not only working but also understood
	* Note: Some of these topics are also discussed in the course Text Mining. Those of you following both courses are likely to save some time here.	



## Week 12

* **Chapter 22**
	* Read Intro (RU), 22.1 (RU), 22.2 (RU), 22.3 (RU), 22.4 (RU), 22.5 (R), Summary (RU), Notes (R)
	* Exercises
		* Investigate the NewsReader system at http://ixa2.si.ehu.es/nrdemo/demo.php . Start with the attached analysed text. Examine what the system thinks it has discovered in the text, which resources it links to, and to which degree the analysis is right and/or useful. If you think you have discovered specific strong or weak points, check your observations with additional text material.

* **Chapter 23**
	* Read Intro (RU), 23.1 (RU), 23.2 (RU), 23.3 (RU), 23.6 (RU), Summary (RU)

* Note: These topics are also discussed in the course Text Mining. Those of you following both courses are likely to save some time here			



## Week 13

* **Chapter 24**
	* Read Intro (RU), 24.1 (RU), 24.2 (RU), 24.3 (R), 24.4 (RU), 24.5 (RU), 24.6 (R), 24.7 (RU), Summary (RU), Notes (R)
	* Exercises
		* The chapter provides information on techniques that could also be used in building a good chatbot. The question is, however, to which degrees these techniques are actually used in "good" state-of-the-art chatbots. By "good" I mean bots that have been acclaimed or have received substantial press coverage, as being remarkable in their field. The two main fields here are a) virtual help desks, meant to provide customer service instead of a human help desk operator, and b) social chatbots, meant to act as a more or less natural conversation partner. Identify one "good" state-of-the-art chatbot of each type, and investigate it. Try to both evaluate its capabilities, and estimate which technologies from the chapter are being employed. Do not choose Google's new chatbot, which, even though it did get quite a lot of media coverage, is as yet not focussed enough to hold up a longer conversation. Another alternative which is acceptable, is an agent character in a computer game who is able to hold conversations.

## Week 14

* **Chapter 25**
	* Read completely (RU)
	* Exercises
		* Examine the quality of Google Translate (translate.google.com). Make a small test corpus with English sentances. You can reuse the test material you used earlier for e.g. semantics. Also include some syntactic constructions you expect to be problematic, and some examples of metaphors and fixed expressions. Translate to Dutch or your mother tongue if not Dutch. When checking the translations, move you mouse over the translation to see which phrases were used for the translation, and check out the alternatives that Google suggests. Then try some examples of translation in the opposite direction.

* Note: Machine Translation has seen some interesting developments lately. However, we will (probably) have no time to have a look at these.